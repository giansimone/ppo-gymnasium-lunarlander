"""
Training script for PPO agent in Lunar Lander environment.
"""
from pathlib import Path
from datetime import datetime

import torch

from agent import Agent
from environment import make_vec_env
from utils import load_config, save_config, set_seed


def train(config_filename: Path = Path("config.yaml")):
    """Train the PPO agent in the Lunar Lander environment."""
    config = load_config(config_filename)
    set_seed(config["seed"])

    run_name = "ppo_" + config["env_id"] + "_" + datetime.now().strftime("%Y-%m-%d_%Hh%Mm%Ss")

    log_dir = Path(config["log_dir"])
    run_dir = log_dir / run_name
    run_dir.mkdir(parents=True, exist_ok=True)
    save_config(config.copy(), run_dir / "config.yaml")

    env_id = config["env_id"]
    num_envs = config["num_envs"]
    n_steps = config["n_steps"]

    envs, state_dim, action_dim = make_vec_env(
        env_id,
        num_envs,
        render_mode=None
    )

    print(f"| Environment: {env_id} (x{num_envs})"
          f"| State space: {state_dim}, Action space: {action_dim}")

    agent = Agent(
        n_steps=n_steps,
        num_envs=num_envs,
        state_dim=state_dim,
        action_dim=action_dim,
        hidden_dim=config["hidden_dim"],
        lr=config["learning_rate"],
        gamma=config["gamma"],
        gae_lambda=config["gae_lambda"],
        clip_epsilon=config["clip_epsilon"],
        batch_size=config["batch_size"],
        ppo_epochs=config["ppo_epochs"],
        value_coef=config["value_coef"],
        entropy_coef=config["entropy_coef"],
        max_grad_norm=config["max_grad_norm"],
    )

    num_updates = config["total_timesteps"] // n_steps // num_envs

    print(f"Total timesteps: {config['total_timesteps']}, Total updates: {num_updates}")

    state, _ = envs.reset()

    next_state = state

    for update in range(1, num_updates + 1):

        for _ in range(n_steps):
            action_tensor, log_prob, value = agent.select_action(state)

            action = action_tensor.numpy()

            next_state, reward, terminated, truncated, _ = envs.step(action)

            done = terminated | truncated

            agent.buffer.add(
                torch.tensor(state),
                action_tensor,
                log_prob,
                reward,
                value,
                done,
            )

            state = next_state

        agent.learn(next_state)

        print(f"Update {update}/{num_updates} completed.")

    agent.save_model(run_dir / "final_model.pt")

    envs.close()


if __name__ == "__main__":
    train()
